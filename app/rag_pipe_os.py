import os
import pandas as pd
from typing import List, Dict
from typing_extensions import TypedDict

# Importa√ß√µes do LangChain e LangGraph
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, START

# ==============================================================================
# 1. CONFIGURA√á√ÉO INICIAL E SETUP DO BACKEND (PIPELINE RAG)
# ==============================================================================

# -- CONFIGURA√á√ÉO DO PIPELINE --
persist_directory = "./chroma_db_reparos"
collection_name = "reparos_historico"
csv_path = "./data/resultado_sem_duplicatas.csv"

def get_rag_graph():
    """
    Fun√ß√£o principal que configura e retorna o pipeline RAG compilado (grafo).
    Esta fun√ß√£o encapsula toda a l√≥gica do backend.
    """
    print("Executando setup do Pipeline RAG...")

    # LLM e Embeddings
    llm = ChatOpenAI(model="gpt-4o", temperature=0.0, max_tokens=1000)
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

    # Vetoriza√ß√£o e Indexa√ß√£o do CSV
    if os.path.exists(persist_directory):
        print("üîÅ Carregando banco de dados vetorial de reparos existente...")
        vector_store = Chroma(
            collection_name=collection_name,
            embedding_function=embeddings,
            persist_directory=persist_directory,
        )
    else:
        print(f"üìö Indexando dados do arquivo CSV: {csv_path}...")
        if not os.path.exists(os.path.dirname(csv_path)):
            raise FileNotFoundError(f"ERRO: O diret√≥rio 'data' n√£o foi encontrado.")
        
        try:
            df = pd.read_csv(csv_path)
        except FileNotFoundError:
            raise FileNotFoundError(f"ERRO: O arquivo {csv_path} n√£o foi encontrado.")

        documents = []
        for index, row in df.iterrows():
            content = (
                f"Registro de Manuten√ß√£o OS {row.get('OS', 'N/A')}:\n"
                f"- Tipo do Equipamento: {row.get('TIPO EQUIPAMENTO', 'N/A')}\n"
                f"- Equipamento: {row.get('EQUIPAMENTO', 'N/A')}\n"
                f"- Tipo de Manuten√ß√£o: {row.get('TIPO', 'N/A')}\n"
                f"- Marca: {row.get('MARCA', 'N/A')}\n"
                f"- Tempo da manuten√ß√£o: {row.get('TEMPO', 'N/A')}\n"
                f"- Tipo de Servi√ßo: {row.get('TIPO DE SERVI√áO', 'N/A')}\n"
                f"- Complemento: {row.get('COMPLEMENTO', 'N/A')}\n"
                f"- Solu√ß√£o Aplicada: {row.get('RESOLU√á√ÉO', 'N/A')}"
            )
            doc = Document(
                page_content=content,
                metadata={
                    "os_number": str(row.get('OS', '')),
                    "equipment": str(row.get('EQUIPAMENTO', '')),
                    "type": str(row.get('TIPO', '')),
                    "brand": str(row.get('MARCA', '')),
                }
            )
            documents.append(doc)

        vector_store = Chroma.from_documents(
            documents=documents, embedding=embeddings,
            collection_name=collection_name, persist_directory=persist_directory
        )
        print("‚úÖ Banco de dados vetorial criado e salvo com sucesso!")

    # Defini√ß√£o do Agente (Grafo com LangGraph)
    prompt = ChatPromptTemplate.from_template("""
    Voc√™ √© um assistente s√™nior de manuten√ß√£o t√©cnica, treinado com base em um hist√≥rico de Ordens de Servi√ßo (OS). 
    Sua fun√ß√£o √© analisar problemas relatados e oferecer suporte com sugest√µes √∫teis, claras e t√©cnicas.

    <restricoes>
    - N√£o invente solu√ß√µes ou causas que n√£o estejam relacionadas ao hist√≥rico fornecido.
    - Se n√£o encontrar informa√ß√µes relevantes no contexto, responda educadamente que **n√£o foi poss√≠vel encontrar uma solu√ß√£o com base nos dados dispon√≠veis**.
    - Seja t√©cnico, claro e objetivo.
    </restricoes>

    <permissoes>
    - Voc√™ pode analisar a descri√ß√£o de um problema e buscar casos semelhantes no hist√≥rico de OS.
    - Voc√™ pode sugerir causas prov√°veis, solu√ß√µes e procedimentos t√©cnicos usados em casos semelhantes.
    - Voc√™ pode informar o tempo estimado de execu√ß√£o com base nos registros anteriores.
    - Voc√™ deve sempre indicar que as sugest√µes s√£o baseadas em hist√≥rico e n√£o substituem diagn√≥stico t√©cnico presencial.
    </permissoes>

    ---
    - Equipamento informado: {equipamento}
    - Hist√≥rico da conversa: {history}
    - Descri√ß√£o do problema: {question}
    - Contexto extra√≠do do hist√≥rico de OS: {context}
    - Resposta:
    """)

    class State(TypedDict):
        question: str
        equipamento: str
        context: List[Document]
        chat_history: List[Dict[str, str]]
        answer: str

    def retrieve(state: State):
        search_query = f"Falha no equipamento {state['equipamento']}: {state['question']}"
        docs_with_scores = vector_store.similarity_search_with_score(search_query, k=10)

        # Reranking: ordenar pelos scores mais baixos (mais similares)
        docs_sorted = sorted(docs_with_scores, key=lambda x: x[1])
        top_docs = [doc for doc, _ in docs_sorted[:4]]  # pega os 4 melhores

        state['context'] = top_docs
        return state

    def generate(state: State):
        history_text = "\n".join(
            f"{'Usu√°rio' if msg['role'] == 'user' else 'Assistente'}: {msg['content']}" 
            for msg in state.get("chat_history", [])
        )
        docs_text = "\n\n---\n\n".join(doc.page_content for doc in state["context"])
        messages = prompt.invoke({
            "question": state["question"],
            "equipamento": state["equipamento"],
            "context": docs_text,
            "history": history_text
        })
        response = llm.invoke(messages)
        state['answer'] = response.content
        return state

    graph_builder = StateGraph(State)
    graph_builder.add_node("retrieve", retrieve)
    graph_builder.add_node("generate", generate)
    graph_builder.add_edge(START, "retrieve")
    graph_builder.add_edge("retrieve", "generate")
    
    return graph_builder.compile()